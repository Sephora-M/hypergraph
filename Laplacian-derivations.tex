\documentclass[12pt]{article}\pagestyle{myheadings}

\title{Learning with Hypergraphs}
\author{Sephora Madjiheurem}

\usepackage{amsmath,amssymb,amsthm,amsfonts,graphics}
%The following commands allow us to typeset theorems, propositions, definitions, etc.
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{definition}{Definition}

\renewcommand{\qedsymbol}{\ensuremath{\blacksquare}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}} 
\newcommand{\V}{\mathcal{V}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\lapl}{\mathcal{L}}
\newcommand{\Div}{\text{div}}
\newcommand{\diag}{\text{diag}}


\begin{document}
\maketitle
%Enter your email address.

\section{Hypergraph Laplacian generalization}
\subsection{Non-normalized Laplacian}
About the notation used, refer to \textit{Zhou and al.} section 2.\\ 
Define the \textbf{hyperedge derivative} $\frac{\partial f}{\partial_e} : \E \rightarrow ?? $ for a signal $f: \V \rightarrow \R$ as :
$$\frac{\partial f}{\partial_e} (e) = \bigg( \sqrt[]{w(e)}(f(u)-f(v)) \bigg)_{u,v \in e} $$
Then obtain the \textbf{gradient} $\nabla_{HG} f: \R ^{|\V|} \rightarrow ??$  of a hypergraph signal $f$ :
$$ \nabla_{HG} f = \bigg( \frac{\partial f}{\partial_e}(e) \bigg)_{e \in \E}$$
The \textbf{divergence}, the adjoint of the gradient operator, satisfies $\langle \nabla f, g_e \rangle = \langle f, \Div g_e\rangle$ for a signal $g$ on the hyperedges $g_e(e) = \big( g(u,v) \big )_{\{ u,v\} \in e}$. 
\begin{equation*}
\begin{split}
\langle \nabla f, g \rangle  & = \sum_e \sum_{ \{u,v\} \subseteq e} \sqrt[]{w(e)}(f(u)-f(v)) g(u,v)\\
& = \frac{1}{2}\sum_e \sqrt[]{w(e)}  \sum_{u \in e} \sum_{v \in e} (f(u)g(u,v)-f(v)g(u,v))  \\
& =  \frac{1}{2}\sum_e \sqrt[]{w(e)} \big( \sum_{u \in e} \sum_{v \in e} f(u)g(u,v) - \sum_{u \in e} \sum_{v \in e} f(v)g(u,v) \big) \\
& = \frac{1}{2}\sum_e \sqrt[]{w(e)} \big( \sum_{u \in e} \sum_{v \in e} f(u)g(u,v) - \sum_{v \in e} \sum_{u \in e} f(u)g(v,u) \big) \\
& = \frac{1}{2}\sum_{u \in e} f(u) \sum_e \sqrt[]{w(e)} \big(  \sum_{v \in e} g(u,v) - \sum_{v \in e}  g(v,u) \big) \\
& = \frac{1}{2}\sum_{u} f(u) \sum_e \sqrt[]{w(e)} \big(  \sum_{v } g(u,v)h(e,v) - \sum_{v}  g(v,u)h(e,v) \big)h(e,u) 
\end{split}
\end{equation*}
Thus the divergence is then defined as:
$$(\Div g)(u) = \frac{1}{2} \sum_e \sqrt[]{w(e)} \big(  \sum_{v } \big( g(u,v)-  g(v,u) \big)\big)h(e,v) h(e,u)$$
We now derive the \textbf{hypergraph Laplacian}: $\lapl: \R^{|\V|} \rightarrow \R^{|\V|}$ defined as usual by: $ \lapl f \mapsto -\Div \nabla f$

\begin{equation*}
\begin{split}
\lapl f(u) &= -\Div \nabla f \\
 & = \frac{1}{2} \sum_e  \big(  \sum_{v } \big( \sqrt[]{w(e)}\sqrt[]{w(e)}(f(u)-f(v))-  \sqrt[]{w(e)}\sqrt[]{w(e)}(f(v)-f(u)) \big)\big)h(e,v) h(e,u)\\
 & = \frac{1}{2} \big( \sum_e    \sum_{v }w(e)(f(u)-f(v)- \sum_e    \sum_{v }  w(e)(f(v)-f(u)) \big)h(e,v) h(e,u)\\
 & =  \frac{1}{2} \big( \sum_e    \sum_{v }w(e)(f(u)-f(v)+ \sum_e    \sum_{v }  w(e)(f(u)-f(v)) \big)h(e,v) h(e,u)\\
 & =  \sum_e    \sum_{v }w(e)(f(u)-f(v))h(e,v) h(e,u)\\
 & =  \sum_e \sum_{v }w(e)f(u)h(e,v) h(e,u)- \sum_e \sum_{v }w(e)f(v)h(e,v) h(e,u)\\
 & = f(u)\sum_e w(e) h(e,u)\underbrace{\sum_{v }h(e,v)}_{\delta(e)}- \sum_{v }f(v)\underbrace{\sum_e w(e)h(e,v) h(e,u)}_{\textbf{A}(u,v)}\\
\end{split}
\end{equation*}
And the combinatorial Laplacian follows:
$$ \lapl = \textbf{D}_v' - \textbf{A}$$
where $\textbf{D}_v'$ is a diagonal matrix similar to $\textbf{D}_v$ but using weights $w'(e) = w(e)\delta(e)$ instead and $\textbf{A} = \textbf{HWH}^T$

\subsection{Normalized Laplacian}

Define the \textbf{hyperedge derivative} $\frac{\partial f}{\partial_e} : \E \rightarrow ?? $ for a signal $f: \V \rightarrow \R$ as :
$$\frac{\partial f}{\partial_e} (e) =  \Bigg(\frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}}\bigg(\frac{f(u)}{d(u)}-\frac{f(v)}{d(v)} \bigg) \Bigg)_{u,v \in e} $$
Then obtain the \textbf{gradient} $\nabla_{HG} f: \R ^{|\V|} \rightarrow ??$  of a hypergraph signal $f$ :
$$ \nabla_{HG} f = \bigg( \frac{\partial f}{\partial_e}(e) \bigg)_{e \in \E}$$
The \textbf{divergence}, the adjoint of the gradient operator, satisfies $\langle \nabla f, g_e \rangle = \langle f, \Div g_e\rangle$ for a signal $g$ on the hyperedges $g_e(e) = \big( g(u,v) \big )_{\{ u,v\} \in e}$. 
\begin{equation*}
\begin{split}
\langle \nabla f, g \rangle  & = \sum_e \sum_{ \{u,v\} \subseteq e}\frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}}\bigg(\frac{f(u)}{d(u)}-\frac{f(v)}{d(v)} \bigg) g(u,v)\\
& = \frac{1}{2}\sum_e\frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}}  \sum_{u \in e} \sum_{v \in e} (\frac{f(u)}{d(u)}g(u,v)-\frac{f(v)}{d(v)}g(u,v))  \\
& =  \frac{1}{2}\sum_e \frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}} \big( \sum_{u \in e} \sum_{v \in e} \frac{f(u)}{d(u)}g(u,v) - \sum_{u \in e} \sum_{v \in e} \frac{f(v)}{d(v)}g(u,v) \big) \\
& = \frac{1}{2}\sum_e \frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}} \big( \sum_{u \in e} \sum_{v \in e} \frac{f(u)}{d(u)}g(u,v) - \sum_{v \in e} \sum_{u \in e} \frac{f(u)}{d(u)}g(v,u) \big) \\
& = \frac{1}{2}\sum_{u \in e} \frac{f(u)}{d(u)} \sum_e \frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}} \big(  \sum_{v \in e} g(u,v) - \sum_{v \in e}  g(v,u) \big) \\
& = \frac{1}{2}\sum_{u} \frac{f(u)}{d(u)} \sum_e \frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}} \big(  \sum_{v } g(u,v)h(e,v) - \sum_{v}  g(v,u)h(e,v) \big)h(e,u) 
\end{split}
\end{equation*}
Thus the divergence is then defined as:
$$(\Div g)(u) = \frac{1}{2d(u)} \sum_e \frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}} \big(  \sum_{v } \big( g(u,v)-  g(v,u) \big)\big)h(e,v) h(e,u)$$
We now derive the \textbf{hypergraph Laplacian}: $\lapl: \R^{|\V|} \rightarrow \R^{|\V|}$ defined as usual by: $ \lapl f \mapsto -\Div \nabla f$

\begin{equation*}
\begin{split}
\lapl f(u) &= -\Div \nabla f \\
 & = \frac{1}{2} \sum_e  \big(  \sum_{v } \big( \frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}}\frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}}(\frac{f(u)}{d(u)}-\frac{f(v)}{d(v)} )-  \frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}}\frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}}(\frac{f(v)}{d(v)}-\frac{f(u)}{d(u)} ) \big)\big)h(e,v) h(e,u)\\
 & = \frac{1}{2} \big( \sum_e    \sum_{v }\frac{w(e)}{\delta(e)}(\frac{f(u)}{d(u)}-\frac{f(v)}{d(v)} - \sum_e    \sum_{v }  \frac{w(e)}{\delta(e)}(\frac{f(v)}{d(v)}-\frac{f(u)}{d(u)}) \big)h(e,v) h(e,u)\\
 & =  \frac{1}{2} \big( \sum_e    \sum_{v }\frac{w(e)}{\delta(e)}(\frac{f(u)}{d(u)}-\frac{f(v)}{d(v)} + \sum_e    \sum_{v }  \frac{w(e)}{\delta(e)}(\frac{f(u)}{d(u)}-\frac{f(v)}{d(v)} ) \big)h(e,v) h(e,u)\\
 & =  \sum_e    \sum_{v }\frac{w(e)}{\delta(e)}(\frac{f(u)}{d(u)}-\frac{f(v)}{d(v)} )h(e,v) h(e,u)\\
 & =  \sum_e \sum_{v }\frac{w(e)}{\delta(e)}\frac{f(u)}{d(u)}h(e,v) h(e,u)- \sum_e \sum_{v }\frac{w(e)}{\delta(e)}\frac{f(v)}{d(v)}h(e,v) h(e,u)\\
 & = \frac{f(u)}{d(u)}\sum_e \frac{w(e)}{\delta(e)} h(e,u)\underbrace{\sum_{v }h(e,v)}_{\delta(e)}- \sum_{v }\frac{f(v)}{d(v)}\sum_e \frac{w(e)}{\delta(e)}h(e,v) h(e,u)\\
  & = \frac{f(u)}{d(u)}\sum_e w(e) h(e,u)- \sum_{v }f(v)\sum_e \frac{1}{\sqrt[]{d(v)}}h(e,u)  \frac{w(e)}{\delta(e)} h(e,v) \frac{1}{\sqrt[]{d(v)}}\\
\end{split}
\end{equation*}
And the combinatorial Laplacian follows:
$$ \lapl = \textbf{I} - \textbf{$\Theta$}$$
where $\textbf{I}$ is the identity matrix and $\textbf{$\Theta$} = \textbf{$D_V^{-\frac{1}{2}}HWD_e^{-1}H^TD_V^{-\frac{1}{2}}$}$

\subsection{Another Laplacian}

Using the same procedure using the following hyperedge derivative
$$\frac{\partial f}{\partial_e} (e) =  \Bigg(\frac{\sqrt[]{w(e)}}{\sqrt[]{\delta(e)}}(f(u)-f(v) ) \Bigg)_{u,v \in e} $$
We can derive the Laplacian $$ \lapl = \textbf{D}_v - \textbf{A'}$$
where  $\textbf{A'} = \textbf{$HWD_e^{-1}H^T$}$


\end{document}





